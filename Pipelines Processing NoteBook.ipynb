{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "useeplecwhnwd4i3qqsb",
   "authorId": "8828518807709",
   "authorName": "PAULHAPPY",
   "authorEmail": "",
   "sessionId": "35a98b57-d075-42b7-a51c-126e8b7f4b32",
   "lastEditTime": 1745985528631
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3094b063-9ef4-4a7a-b6ec-cf7422103148",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8002cb60-2ce5-4129-83ab-9dca4f40158e",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "import re\nfrom snowflake.snowpark.functions import col, lit\nimport snowflake.snowpark as snowpark\nfrom typing import Optional\n\nclass BronzeIngestor:\n    def __init__(\n        self,\n        session: snowpark.Session,\n        stage_path: str,             # e.g., @DB.SCHEMA.STAGE/datalake/bronze/crawler/metadata/user_post\n        bronze_table: str,\n        meta_table: str = \"PROCESSING_DATE\"\n    ):\n        self.session = session\n        self.stage = stage_path.rstrip(\"/\")   # Ensure no trailing slash\n        self.bronze = bronze_table\n        self.meta = meta_table\n\n        # Create processing metadata table if not exists\n        self.session.sql(\"USE DATABASE TECHTEST\").collect()\n        self.session.sql(\"USE SCHEMA BACKEND_DEV_DE_2025_PAULHAPPY\").collect()\n        \n        self.session.sql(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {self.meta} (\n            source STRING PRIMARY KEY,\n            last_processed TIMESTAMP_LTZ\n        )\n        \"\"\").collect()\n\n    def get_last_processed(self) -> Optional[str]:\n        df = self.session.table(self.meta).filter(col(\"source\") == self.bronze)\n        row = df.collect()\n        return row[0][\"LAST_PROCESSED\"] if row else None\n\n    def update_last_processed(self, ts: str):\n        self.session.sql(f\"\"\"\n        MERGE INTO {self.meta} AS m\n        USING (SELECT '{ts}'::TIMESTAMP_LTZ AS last_processed) AS v\n        ON m.source = '{self.bronze}'\n        WHEN MATCHED THEN UPDATE SET last_processed = v.last_processed\n        WHEN NOT MATCHED THEN INSERT (source, last_processed)\n        VALUES ('{self.bronze}', v.last_processed)\n        \"\"\").collect()\n\n    def ingest(self):\n        last_ts = self.get_last_processed()\n\n        # Step 1: List all JSON.gz files\n        file_list = self.session.sql(f\"LIST {self.stage}\").collect()\n        print(file_list)\n        json_files = [\n            row[\"name\"]\n            for row in file_list\n            if row[\"name\"].endswith(\".json.gz\")\n        ]\n\n        all_dfs = []\n\n        for file_path in json_files:\n            # Match: user_post/{ingestion_ts}/{platform}/{author}/{file}.json.gz\n            match = re.search(\n                r\"backend_dev_de_2025_stage_paulhappy/datalake/bronze/crawler/metadata/user_post/([^/]+)/([^/]+)/([^/]+)/[^/]+\\.json\\.gz\",\n                file_path\n            )\n\n            if not match:\n                continue\n\n            ingestion_ts_raw, platform, author = match.groups()\n\n            # Convert to proper TS format\n            ingestion_ts = ingestion_ts_raw.replace(\"T\", \" \") \\\n                                           .replace(\"-\", \":\", 2) \\\n                                           .replace(\"-\", \"\", 1)\n\n            # Compare ingestion date\n            if last_ts and ingestion_ts <= str(last_ts):\n                continue\n\n            df = self.session.read.option(\"compression\", \"gzip\").json(f\"@{file_path}\")\n            df = df.with_column(\"ingestion_date_timestamp\", lit(ingestion_ts))\n            df = df.with_column(\"platform\", lit(platform))\n            df = df.with_column(\"author\", lit(author))\n            all_dfs.append(df)\n\n        if not all_dfs:\n            print(\"No new files to ingest.\")\n            return\n\n        # Combine all into one DataFrame\n        full_df = all_dfs[0]\n        for df in all_dfs[1:]:\n            full_df = full_df.union(df)\n\n        result_df = full_df.select(\n            col(\"author_id\"),\n            col(\"text\"),\n            col(\"timestamp\"),\n            col(\"likes\"),\n            col(\"reposts\"),\n            col(\"comments\"),\n            col(\"media_urls\"),\n            col(\"media_local_paths\"),\n            col(\"ingestion_date_timestamp\"),\n            col(\"platform\"),\n            col(\"author\")  # from path\n        )\n\n        # Ensure bronze table exists\n        self.session.sql(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {self.bronze} (\n            author_id STRING,\n            text STRING,\n            timestamp STRING,\n            likes NUMBER,\n            reposts NUMBER,\n            comments NUMBER,\n            media_urls ARRAY,\n            media_local_paths ARRAY,\n            ingestion_date_timestamp STRING,\n            platform STRING,\n            author STRING\n        )\n        \"\"\").collect()\n\n        result_df.write.mode(\"append\").save_as_table(self.bronze)\n\n        max_ts = result_df.agg({\"ingestion_date_timestamp\": \"max\"}).collect()[0][0]\n        self.update_last_processed(max_ts)\n\n        print(f\"Ingested up to {max_ts}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44d20e5e-050b-447e-b632-5d2a1efc9c37",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "class SilverTransformer:\n    def __init__(self, session: snowpark.Session, bronze_table: str, silver_table: str):\n        self.session = session\n        self.bronze = bronze_table\n        self.silver = silver_table\n\n    def run(self):\n        df = self.session.table(self.bronze)\n        clean = df.select(\n            col(\"author_id\"),\n            col(\"text\"),\n            # convert ISO string → TIMESTAMP_LTZ\n            to_timestamp_ltz(col(\"timestamp\"),\n                             \"YYYY-MM-DD\\\"T\\\"HH24:MI:SS\").as_(\"timestamp\"),\n            col(\"likes\"),\n            col(\"reposts\"),\n            col(\"comments\"),\n            col(\"media_urls\"),\n            col(\"media_local_paths\"),\n            to_timestamp_ltz(col(\"ingestion_date_timestamp\"),\n                             \"YYYY-MM-DD\\\"T\\\"HH24:MI:SS\")\n              .as_(\"ingestion_date_timestamp\"),\n            col(\"platform\")\n        ).drop_duplicates()\n\n        # write out\n        self.session.sql(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {self.silver} LIKE {self.bronze};\n        \"\"\").collect()\n        clean.write.mode(\"overwrite\").save_as_table(self.silver)\n        print(f\"Silver table {self.silver} updated.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0a199d3-5333-447f-afb3-cef941a39862",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "class GoldAggregator:\n    def __init__(self, session: snowpark.Session, silver_table: str):\n        self.session = session\n        self.silver = silver_table\n\n    def build(self):\n        self.session.sql(f\"\"\"\n            CREATE OR REPLACE TABLE gold_crawler_data AS\n                WITH base AS (\n                  SELECT\n                    author_id,\n                    text,\n                    timestamp,\n                    (likes + reposts + comments) AS interactions,\n                    DATE_TRUNC('WEEK', timestamp) AS week_start\n                  FROM silver.crawler_data\n                ),\n                author_top AS (\n                  -- top post per author (all‐time)\n                  SELECT author_id,\n                         text AS top_post_text,\n                         interactions AS top_post_interactions\n                  FROM (\n                    SELECT author_id, text, interactions,\n                           ROW_NUMBER() OVER (\n                             PARTITION BY author_id\n                             ORDER BY interactions DESC\n                           ) AS rn\n                    FROM base\n                  )\n                  WHERE rn = 1\n                ),\n                weekly_top AS (\n                  -- top post per author per week\n                  SELECT author_id,\n                         week_start,\n                         text  AS weekly_top_post_text,\n                         interactions AS weekly_top_post_interactions\n                  FROM (\n                    SELECT author_id,\n                           week_start,\n                           text,\n                           interactions,\n                           ROW_NUMBER() OVER (\n                             PARTITION BY author_id, week_start\n                             ORDER BY interactions DESC\n                           ) AS rn\n                    FROM base\n                  )\n                  WHERE rn = 1\n                )\n                SELECT\n                  pc.author_id,\n                  w.week_start,\n                  at.top_post_text,\n                  at.top_post_interactions,\n                  w.weekly_top_post_text,\n                  w.weekly_top_post_interactions\n                FROM author_top  at \n                LEFT JOIN weekly_top  w  ON w.author_id   = at.author_id\n            ;\"\"\").collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ce37b7d-63ed-4b14-8b31-abbd0f5c565a",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def main(session: snowpark.Session):\n    # adjust these names to match your Snowflake objects\n    BRONZE_TABLE = \"bronze_crawler_data\"\n    SILVER_TABLE = \"silver_crawler_data\"\n    STAGE_NAME   = '@\"TECHTEST\".\"BACKEND_DEV_DE_2025_PAULHAPPY\".\"BACKEND_DEV_DE_2025_STAGE_PAULHAPPY\"/datalake/bronze/crawler/metadata/user_post'                       \n    ING_META_TBL = \"PROCESSING_DATE\"\n\n    # 1) Bronze\n    bi = BronzeIngestor(\n        session,\n        stage_path=STAGE_NAME,\n        bronze_table=BRONZE_TABLE,\n        meta_table=ING_META_TBL\n    )\n    bi.ingest()\n\n    # 2) Silver\n    st = SilverTransformer(session, bronze_table=BRONZE_TABLE, silver_table=SILVER_TABLE)\n    st.run()\n\n    # 3) Gold\n    ga = GoldAggregator(session, silver_table=SILVER_TABLE)\n    ga.build()\n\n    return \"Pipeline complete!\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44fc9bff-ee1f-4513-bea6-5a1a392423ad",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "LIST '@\"TECHTEST\".\"BACKEND_DEV_DE_2025_PAULHAPPY\".\"BACKEND_DEV_DE_2025_STAGE_PAULHAPPY\"/datalake/bronze/crawler/metadata/user_post'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5de1963-6b8a-46cf-8b8f-2a297abb0b30",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "main(session)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac27c2fe-3ce6-4bd6-86ef-90a9aecb9ffd",
   "metadata": {
    "language": "sql",
    "name": "cell4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}